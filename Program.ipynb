{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analisis Sentimen Terhadap Presiden Jokowi</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as regex\n",
    "import pandas as pd\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Data berikut diperoleh dari komentar warganet terhadap Presiden Jokowi di Instagram. Pengumpulan data dilakukan dengan melakukan pencarian berdasarkan tag #jokowi</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'pos', 'kalimat': 'MAJU TRUSS PAK JOKOWI!!!'}\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"dataset jokowi.csv\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "arr_data = []\n",
    "for index, row in data.iterrows():\n",
    "    map_data = {}\n",
    "    r = row[0].split(\";\")\n",
    "    map_data[\"label\"] = r[0]\n",
    "    map_data[\"kalimat\"] = r[1]\n",
    "    \n",
    "    if not pd.isnull(row[1]):\n",
    "        map_data[\"kalimat\"] += row[1]\n",
    "    arr_data.append(map_data)\n",
    "print(arr_data[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><i>Preprocessing</i></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Symbol Removal</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAJU TRUSS PAK JOKOWI\n"
     ]
    }
   ],
   "source": [
    "symbolless = []\n",
    "for d in arr_data:\n",
    "    sentence = regex.sub(\"[^a-zA-Z0-9 ]\",\"\",d[\"kalimat\"])\n",
    "    symbolless.append(sentence)\n",
    "print(symbolless[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Casefolding</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maju truss pak jokowi\n"
     ]
    }
   ],
   "source": [
    "cases = []\n",
    "for s in symbolless:\n",
    "    sentence = s.lower()\n",
    "    cases.append(sentence)\n",
    "print(cases[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Stopword Removal</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maju truss jokowi\n"
     ]
    }
   ],
   "source": [
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.create_stop_word_remover()\n",
    "\n",
    "removed_cases = []\n",
    "\n",
    "for c in cases:\n",
    "    c = stopwords.remove(c)\n",
    "    removed_cases.append(c)\n",
    "print(removed_cases[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tokenization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maju', 'truss', 'jokowi']\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "for c in removed_cases:\n",
    "    token_data = c.split(\" \")\n",
    "    tokens.append(token_data)\n",
    "print(tokens[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Stemming</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maju', 'truss', 'jokowi']\n"
     ]
    }
   ],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "stem_word = []\n",
    "\n",
    "for t in tokens:\n",
    "    comment = []\n",
    "    for w in t:\n",
    "        stem_data = stemmer.stem(w)\n",
    "        comment.append(stem_data)\n",
    "    stem_word.append(comment)\n",
    "print(stem_word[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Naive Bayes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_corpus = []\n",
    "APLHA = 1\n",
    "def naivebayes(grams=[]):\n",
    "    if not grams:\n",
    "        return 0\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
